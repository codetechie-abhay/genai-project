{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Groq Chatbot with MemorySaver\n",
    "\n",
    "A standalone chatbot using only Groq API and MemorySaver for conversation memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (0.37.1)\n",
      "Requirement already satisfied: langgraph in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (1.0.8)\n",
      "Requirement already satisfied: langchain-groq in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (1.1.2)\n",
      "Collecting tavily\n",
      "  Downloading tavily-1.1.0.tar.gz (5.1 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from groq) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from groq) (2.12.5)\n",
      "Requirement already satisfied: sniffio in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from groq) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
      "Requirement already satisfied: certifi in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from httpx<1,>=0.23.0->groq) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
      "Requirement already satisfied: langchain-core>=0.1 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from langgraph) (1.2.13)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from langgraph) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from langgraph) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from langgraph) (0.3.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from langgraph) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph) (1.12.2)\n",
      "Requirement already satisfied: orjson>=3.10.1 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from langchain-core>=0.1->langgraph) (0.6.4)\n",
      "Requirement already satisfied: packaging>=23.2.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from langchain-core>=0.1->langgraph) (0.13.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
      "Requirement already satisfied: aiohttp in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from tavily) (3.13.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.6.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from aiohttp->tavily) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from aiohttp->tavily) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from aiohttp->tavily) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from aiohttp->tavily) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from aiohttp->tavily) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from aiohttp->tavily) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in C:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (from aiohttp->tavily) (1.22.0)\n",
      "Building wheels for collected packages: tavily\n",
      "  Building wheel for tavily (pyproject.toml): started\n",
      "  Building wheel for tavily (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for tavily: filename=tavily-1.1.0-py3-none-any.whl size=6170 sha256=2953f0fb921a109bc1c7733d51f25600286bd8a5e7085dbd5e0515cdf9f5d3be\n",
      "  Stored in directory: c:\\users\\abhay\\appdata\\local\\pip\\cache\\wheels\\51\\08\\fd\\a42b1feea25355e9e6357061510b277422f42ee1f044aa24d3\n",
      "Successfully built tavily\n",
      "Installing collected packages: tavily\n",
      "Successfully installed tavily-1.1.0\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install groq langgraph langchain-groq tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from typing import TypedDict, List\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Groq API key\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state for our chatbot\n",
    "class State(TypedDict):\n",
    "    messages: List[HumanMessage | AIMessage | SystemMessage]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "memory saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MemorySaver\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Create the chatbot function\n",
    "def chatbot_node(state: State):\n",
    "    \"\"\"Process messages and generate response using Groq\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Convert messages for Groq API\n",
    "    groq_messages = []\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            groq_messages.append({\"role\": \"user\", \"content\": msg.content})\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            groq_messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "        elif isinstance(msg, SystemMessage):\n",
    "            groq_messages.append({\"role\": \"system\", \"content\": msg.content})\n",
    "    \n",
    "    # Get response from Groq\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=groq_messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    # Add AI response to messages\n",
    "    ai_response = AIMessage(content=response.choices[0].message.content)\n",
    "    messages.append(ai_response)\n",
    "    \n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "graph = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"chatbot\", chatbot_node)\n",
    "\n",
    "# Add edges\n",
    "graph.add_edge(START, \"chatbot\")\n",
    "graph.add_edge(\"chatbot\", END)\n",
    "\n",
    "# Compile with memory\n",
    "app = graph.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Simple Groq Chatbot with MemorySaver initialized!\n",
      "Type 'quit' to exit or 'new' to start a new conversation thread.\n"
     ]
    }
   ],
   "source": [
    "# Initialize conversation with system message\n",
    "def create_conversation(thread_id: str = \"default\"):\n",
    "    \"\"\"Create or resume a conversation\"\"\"\n",
    "    initial_state = {\n",
    "        \"messages\": [\n",
    "            SystemMessage(content=\"You are a helpful AI assistant. Remember our conversation and provide contextually relevant responses.\")\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    return app.invoke(initial_state, config)\n",
    "\n",
    "# Start a conversation\n",
    "print(\"ü§ñ Simple Groq Chatbot with MemorySaver initialized!\")\n",
    "print(\"Type 'quit' to exit or 'new' to start a new conversation thread.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ AI: The Prime Minister of India is a significant position in the country's government. As of my knowledge cutoff, the Prime Minister of India is Narendra Modi. He has been in office since May 26, 2014. Would you like to know more about his policies, achievements, or something else related to the Indian government?\n",
      "ü§ñ AI: Hello again! It's nice to continue our conversation. We were just discussing the Prime Minister of India, Narendra Modi. Is there something specific you'd like to know or talk about, or would you like to start a new topic? I'm here to help and chat with you!\n",
      "ü§ñ AI: It seems like you didn't type anything. That's okay! Let's try again. Is there something on your mind that you'd like to talk about, or would you like me to suggest some conversation topics? I'm here to listen and help.\n",
      "ü§ñ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Interactive chat function\n",
    "def chat_interactive():\n",
    "    thread_id = \"default\"\n",
    "    \n",
    "    # Initialize conversation\n",
    "    state = create_conversation(thread_id)\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nüë§ You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"ü§ñ Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == 'new':\n",
    "            thread_id = input(\"Enter thread ID (or press Enter for default): \").strip() or f\"thread_{len(memory.storage)}\"\n",
    "            state = create_conversation(thread_id)\n",
    "            config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "            print(f\"ü§ñ Started new conversation thread: {thread_id}\")\n",
    "            continue\n",
    "        \n",
    "        if user_input.lower() == 'threads':\n",
    "            print(f\"üìù Active threads: {list(memory.storage.keys())}\")\n",
    "            continue\n",
    "        \n",
    "        # Add user message and get response\n",
    "        user_message = HumanMessage(content=user_input)\n",
    "        \n",
    "        # Update state with user message\n",
    "        current_state = {\"messages\": state[\"messages\"] + [user_message]}\n",
    "        \n",
    "        # Get AI response\n",
    "        result = app.invoke(current_state, config)\n",
    "        \n",
    "        # Update state\n",
    "        state = result\n",
    "        \n",
    "        # Print AI response\n",
    "        ai_response = result[\"messages\"][-1].content\n",
    "        print(f\"ü§ñ AI: {ai_response}\")\n",
    "\n",
    "# Start chatting\n",
    "chat_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "### MemorySaver Integration:\n",
    "- **Automatic Memory**: Conversation history is automatically saved\n",
    "- **Thread Management**: Multiple conversation threads supported\n",
    "- **Persistence**: Memory persists across notebook sessions\n",
    "\n",
    "### Groq AI Integration:\n",
    "- **Llama 3.3 70B**: Uses Groq's fast Llama model\n",
    "- **Context Awareness**: Remembers previous messages in the thread\n",
    "- **Natural Conversation**: Maintains conversational flow\n",
    "\n",
    "### Commands:\n",
    "- `quit`/`exit`: End the chat\n",
    "- `new`: Start a new conversation thread\n",
    "- `threads`: List all active conversation threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ† Enhancing Your Chatbot with Tools for Real-Time Data\n",
    "#1Ô∏è‚É£ Problem with Static LLMs\n",
    "‚Ä¢\tYour LLM is trained only up to a fixed date.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Complete LangGraph Agent Project\n",
    "\n",
    "## Building a Smart Chatbot with Tools Integration\n",
    "\n",
    "This section demonstrates the complete workflow of building a LangGraph agent with:\n",
    "- State management\n",
    "- Tool integration  \n",
    "- Conditional routing\n",
    "- Memory persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Complete LangGraph Agent with Tools ready!\n",
      "üîß Available tools: ['tavily_search', 'get_current_time', 'calculate']\n",
      "üìù The agent will automatically decide when to use tools!\n"
     ]
    }
   ],
   "source": [
    "# Complete LangGraph Chatbot with Tools Integration\n",
    "from langchain_core.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing import Annotated, Literal\n",
    "import json\n",
    "\n",
    "# 1Ô∏è‚É£ Create Enhanced State with Tool Results\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[HumanMessage | AIMessage | SystemMessage], \"Conversation messages\"]\n",
    "    tool_calls: List[str]  # Track which tools were called\n",
    "    query_type: str  # Track query category for routing\n",
    "\n",
    "# 2Ô∏è‚É£ Create Tools List\n",
    "@tool\n",
    "def tavily_search_tool(query: str) -> str:\n",
    "    \"\"\"Search the web for current information about any topic. Use this for news, current events, or real-time information.\"\"\"\n",
    "    try:\n",
    "        if 'tavily_client' not in globals():\n",
    "            return \"‚ùå Tavily client not initialized. Please set up Tavily API key first.\"\n",
    "        \n",
    "        response = tavily_client.search(\n",
    "            query=query,\n",
    "            max_results=3,\n",
    "            include_answer=True,\n",
    "            search_depth=\"basic\"\n",
    "        )\n",
    "        \n",
    "        if response.get('answer'):\n",
    "            return f\"üîç Search Results: {response['answer']}\"\n",
    "        elif response.get('results'):\n",
    "            top_result = response['results'][0]\n",
    "            return f\"üîç Found: {top_result['title']}\\nüìÑ {top_result['content'][:200]}...\"\n",
    "        else:\n",
    "            return \"No search results found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def get_current_time_tool() -> str:\n",
    "    \"\"\"Get the current date and time. Use this when user asks about time or date.\"\"\"\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    return f\"üìÖ Current time: {now.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "@tool\n",
    "def calculate_tool(expression: str) -> str:\n",
    "    \"\"\"Calculate mathematical expressions. Use for basic math operations like addition, subtraction, multiplication, division.\"\"\"\n",
    "    try:\n",
    "        # Safe evaluation of basic math\n",
    "        allowed_chars = set('0123456789+-*/.() ')\n",
    "        if not all(c in allowed_chars for c in expression):\n",
    "            return \"‚ùå Invalid characters in expression. Only use numbers, +, -, *, /, (, )\"\n",
    "        \n",
    "        result = eval(expression)\n",
    "        return f\"üßÆ {expression} = {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Calculation error: {str(e)}\"\n",
    "\n",
    "# 3Ô∏è‚É£ Create Tools List\n",
    "tools = [tavily_search_tool, get_current_time_tool, calculate_tool]\n",
    "\n",
    "# 4Ô∏è‚É£ Initialize LLM and Bind Tools\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.1,  # Lower temperature for better tool calling\n",
    "    api_key=os.environ[\"GROQ_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Bind tools to LLM to create an Agent\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# 5Ô∏è‚É£ Create Chatbot Node\n",
    "def chatbot_node(state: AgentState):\n",
    "    \"\"\"Main chatbot node that decides whether to use tools or answer directly.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Get response from LLM with tools\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Track if tools were called\n",
    "    tool_calls = [tool_call[\"name\"] for tool_call in response.tool_calls] if response.tool_calls else []\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"tool_calls\": tool_calls,\n",
    "        \"query_type\": \"tool_call\" if tool_calls else \"direct_response\"\n",
    "    }\n",
    "\n",
    "# 6Ô∏è‚É£ Create Tool Node (built-in from LangGraph)\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# 7Ô∏è‚É£ Create Conditional Edge for Routing\n",
    "def route_to_tools(state: AgentState) -> Literal[\"tools\", \"chatbot\"]:\n",
    "    \"\"\"Route to tools if LLM made tool calls, otherwise continue.\"\"\"\n",
    "    if state.get(\"tool_calls\"):\n",
    "        return \"tools\"\n",
    "    return \"chatbot\"\n",
    "\n",
    "# 8Ô∏è‚É£ Build the Complete Graph\n",
    "agent_graph = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "agent_graph.add_node(\"chatbot\", chatbot_node)\n",
    "agent_graph.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add edges\n",
    "agent_graph.add_edge(START, \"chatbot\")\n",
    "agent_graph.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_to_tools,\n",
    "    {\"tools\": \"tools\", \"chatbot\": END}\n",
    ")\n",
    "agent_graph.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "# Compile with memory\n",
    "agent_app = agent_graph.compile(checkpointer=memory)\n",
    "\n",
    "print(\"ü§ñ Complete LangGraph Agent with Tools ready!\")\n",
    "print(f\"üîß Available tools: {[tool.name for tool in tools]}\")\n",
    "print(\"üìù The agent will automatically decide when to use tools!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ LangGraph Agent with Tools ready!\n",
      "üí° Try asking me to:\n",
      "   - Search for 'latest AI news'\n",
      "   - Calculate '25 * 4 + 10'\n",
      "   - Get current time\n",
      "   - Or just chat normally!\n",
      "Type 'quit' to exit.\n",
      "\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=tavily_search{\"query\": \"AI advancements\"}</function>'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîß Tools used: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(result[\u001b[33m'\u001b[39m\u001b[33mtool_calls\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Start the agent chat\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mchat_with_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mchat_with_agent\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     43\u001b[39m current_state = {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [user_message]}\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Invoke agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m result = \u001b[43magent_app\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Print AI response\u001b[39;00m\n\u001b[32m     49\u001b[39m ai_response = result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\langgraph\\pregel\\main.py:3071\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3068\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3069\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3071\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3085\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\langgraph\\pregel\\main.py:2646\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2644\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2645\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2646\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2656\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mchatbot_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     75\u001b[39m messages = state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Get response from LLM with tools\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m response = \u001b[43mllm_with_tools\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Track if tools were called\u001b[39;00m\n\u001b[32m     81\u001b[39m tool_calls = [tool_call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tool_call \u001b[38;5;129;01min\u001b[39;00m response.tool_calls] \u001b[38;5;28;01mif\u001b[39;00m response.tool_calls \u001b[38;5;28;01melse\u001b[39;00m []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5695\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5688\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5689\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5690\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5693\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5694\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5695\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5696\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5697\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5698\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5699\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\langchain_groq\\chat_models.py:621\u001b[39m, in \u001b[36mChatGroq._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    616\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    617\u001b[39m params = {\n\u001b[32m    618\u001b[39m     **params,\n\u001b[32m    619\u001b[39m     **kwargs,\n\u001b[32m    620\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:461\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    243\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    301\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    302\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    304\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\groq\\_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=tavily_search{\"query\": \"AI advancements\"}</function>'}}",
      "During task with name 'chatbot' and id '0fec47c2-6f77-63c8-0272-438cb210c40e'"
     ]
    }
   ],
   "source": [
    "# Interactive Agent Chat with Tools\n",
    "def chat_with_agent():\n",
    "    \"\"\"Chat with the LangGraph agent that has tool access.\"\"\"\n",
    "    thread_id = \"agent_thread\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    # Initialize with system message\n",
    "    initial_state = {\n",
    "        \"messages\": [\n",
    "            SystemMessage(content=\"\"\"You are a helpful AI assistant with access to tools. \n",
    "            Use tools when appropriate:\n",
    "            - Use tavily_search_tool for current information, news, or real-time data\n",
    "            - Use get_current_time_tool when asked about time or date\n",
    "            - Use calculate_tool for math calculations\n",
    "            Always respond naturally after using tools. Don't explain that you used tools.\"\"\")\n",
    "        ],\n",
    "        \"tool_calls\": [],\n",
    "        \"query_type\": \"direct_response\"\n",
    "    }\n",
    "    \n",
    "    # Start conversation\n",
    "    try:\n",
    "        agent_app.invoke(initial_state, config)\n",
    "        print(\"‚úÖ Agent initialized successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing agent: {e}\")\n",
    "        return\n",
    "    \n",
    "    print(\"ü§ñ LangGraph Agent with Tools ready!\")\n",
    "    print(\"üí° Try asking me to:\")\n",
    "    print(\"   - Search for 'latest AI news'\")\n",
    "    print(\"   - Calculate '25 * 4 + 10'\")\n",
    "    print(\"   - Get current time\")\n",
    "    print(\"   - Or just chat normally!\")\n",
    "    print(\"Type 'quit' to exit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"üë§ You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"ü§ñ Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Add user message and get response\n",
    "            user_message = HumanMessage(content=user_input)\n",
    "            \n",
    "            # Get current state and add user message\n",
    "            current_state = {\"messages\": [user_message]}\n",
    "            \n",
    "            # Invoke agent\n",
    "            result = agent_app.invoke(current_state, config)\n",
    "            \n",
    "            # Print AI response\n",
    "            ai_response = result[\"messages\"][-1].content\n",
    "            print(f\"ü§ñ Agent: {ai_response}\")\n",
    "            \n",
    "            # Show tool usage info\n",
    "            if result.get(\"tool_calls\"):\n",
    "                print(f\"üîß Tools used: {', '.join(result['tool_calls'])}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            print(\"Please try again or check your API keys.\")\n",
    "\n",
    "# Start the agent chat\n",
    "chat_with_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Complete Agent Workflow\n",
    "def test_agent_workflow():\n",
    "    \"\"\"Test the agent with different types of queries.\"\"\"\n",
    "    thread_id = \"test_agent\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    test_queries = [\n",
    "        \"What is 15 * 8?\",\n",
    "        \"Search for latest AI developments\",\n",
    "        \"What time is it now?\",\n",
    "        \"Tell me a joke\",\n",
    "        \"Calculate (100 + 50) / 3\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize\n",
    "    initial_state = {\n",
    "        \"messages\": [\n",
    "            SystemMessage(content=\"You are a helpful assistant with tools.\")\n",
    "        ],\n",
    "        \"tool_calls\": [],\n",
    "        \"query_type\": \"direct_response\"\n",
    "    }\n",
    "    \n",
    "    agent_app.invoke(initial_state, config)\n",
    "    \n",
    "    print(\"üß™ Testing Agent Workflow:\\n\")\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"Test {i}: {query}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        user_message = HumanMessage(content=query)\n",
    "        result = agent_app.invoke({\"messages\": [user_message]}, config)\n",
    "        \n",
    "        ai_response = result[\"messages\"][-1].content\n",
    "        print(f\"Response: {ai_response}\")\n",
    "        \n",
    "        if result.get(\"tool_calls\"):\n",
    "            print(f\"Tools used: {result['tool_calls']}\")\n",
    "        else:\n",
    "            print(\"Tools used: None (direct response)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Run the test\n",
    "test_agent_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tavily client initialized successfully!\n",
      "üîç Tavily Search integration ready!\n",
      "Use search_web('your query') to search the web.\n",
      "Run test_search() to test the functionality.\n"
     ]
    }
   ],
   "source": [
    "# Tavily Search Integration\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Initialize Tavily client\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")\n",
    "\n",
    "try:\n",
    "    tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "    print(\"‚úÖ Tavily client initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing Tavily client: {e}\")\n",
    "    print(\"Please make sure you have the correct API key and internet connection.\")\n",
    "\n",
    "def search_web(query: str, max_results: int = 5) -> str:\n",
    "    \"\"\"Search the web using Tavily and return formatted results\"\"\"\n",
    "    try:\n",
    "        if 'tavily_client' not in globals():\n",
    "            return \"‚ùå Tavily client not initialized. Please check your API key.\"\n",
    "            \n",
    "        response = tavily_client.search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            search_depth=\"basic\",\n",
    "            include_answer=True,\n",
    "            include_raw_content=False\n",
    "        )\n",
    "        \n",
    "        if not response.get('results'):\n",
    "            return \"No search results found.\"\n",
    "        \n",
    "        search_summary = f\"üîç Search results for '{query}':\\n\\n\"\n",
    "        \n",
    "        # Add answer if available\n",
    "        if response.get('answer'):\n",
    "            search_summary += f\"üí° Answer: {response['answer']}\\n\\n\"\n",
    "        \n",
    "        # Add top results\n",
    "        for i, result in enumerate(response['results'][:max_results], 1):\n",
    "            search_summary += f\"{i}. {result['title']}\\n\"\n",
    "            search_summary += f\"   üìÑ {result['content'][:200]}...\\n\"\n",
    "            search_summary += f\"   üîó {result['url']}\\n\\n\"\n",
    "        \n",
    "        return search_summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Search error: {str(e)}\"\n",
    "\n",
    "# Test the search function\n",
    "def test_search():\n",
    "    \"\"\"Test Tavily search functionality\"\"\"\n",
    "    try:\n",
    "        test_queries = [\n",
    "            \"latest AI news 2024\",\n",
    "            \"Python programming tips\",\n",
    "            \"machine learning tutorials\"\n",
    "        ]\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            results = search_web(query, max_results=3)\n",
    "            print(results)\n",
    "            print(f\"{'='*50}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test search error: {e}\")\n",
    "\n",
    "print(\"üîç Tavily Search integration ready!\")\n",
    "print(\"Use search_web('your query') to search the web.\")\n",
    "print(\"Run test_search() to test the functionality.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"üîç Search results for 'you have info till which year?':\\n\\nüí° Answer: The Information Age began in the mid-20th century, around the 1970s, and continues to evolve today.\\n\\n1. Information Age - Simple English Wikipedia, the free encyclopedia\\n   üìÑ The Information Age is a historical period that began in the middle of the 20th century. It is defined by a quick change from older industries,...\\n   üîó https://simple.wikipedia.org/wiki/Information_Age\\n\\n2. What is the Information Age? - Quora\\n   üìÑ It's the Age when your phone, computer, TV, refrigerator and commode collect data about you, so that companies can sell you more stuff....\\n   üîó https://www.quora.com/What-is-the-Information-Age\\n\\n3. If You Think We're Still in the Age of Information, Think Again\\n   üìÑ Yes, the information age is still in the process of evolving, and we are, to some degree, still in it. This fact, most people already know....\\n   üîó https://www.linkedin.com/pulse/you-think-were-still-age-information-again-mike-doria\\n\\n4. Information age\\n   üìÑ The Information age also commonly known as the Computer Age or Digital Age, is a period in human history characterized by the shift from traditional industry....\\n   üîó https://en.wikiquote.org/wiki/Information_age\\n\\n5. When did the Information Age begin? - Quora\\n   üìÑ It will always be difficult to pinpoint an exact year for what someone considers to be an ‚Äúage‚Äù. Partly because it depends on what sort of...\\n   üîó https://www.quora.com/When-did-the-Information-Age-begin-1\\n\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_web(\"you have info till which year?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response: Hello Abhay! Nice to meet you. I'm glad you introduced yourself. It's great to know that you're an MTech student of Computer Science and Engineering at GBPIET. How's your experience been so far? Are you enjoying your studies and research? Is there anything specific you'd like to talk about or any help you need? I'm here to assist you.\n",
      "Second response: Your name is Abhay Nautiyal.\n",
      "\n",
      "Memory threads: ['default', 'test_thread']\n",
      "Messages in test_thread: 0\n"
     ]
    }
   ],
   "source": [
    "# Test memory functionality\n",
    "def test_memory():\n",
    "    \"\"\"Test the MemorySaver functionality\"\"\"\n",
    "    test_thread = \"test_thread\"\n",
    "    config = {\"configurable\": {\"thread_id\": test_thread}}\n",
    "    \n",
    "    # Create test conversation\n",
    "    initial_state = {\n",
    "        \"messages\": [\n",
    "            SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "            HumanMessage(content=\"Hello! My name is Abhay Nautiyal MTech Student of CSE GBPIET.\"),\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Get first response\n",
    "    result1 = app.invoke(initial_state, config)\n",
    "    print(\"First response:\", result1[\"messages\"][-1].content)\n",
    "    \n",
    "    # Continue conversation\n",
    "    follow_up = {\n",
    "        \"messages\": result1[\"messages\"] + [\n",
    "            HumanMessage(content=\"What's my name?\")\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    result2 = app.invoke(follow_up, config)\n",
    "    print(\"Second response:\", result2[\"messages\"][-1].content)\n",
    "    \n",
    "    # Check memory storage\n",
    "    print(f\"\\nMemory threads: {list(memory.storage.keys())}\")\n",
    "    print(f\"Messages in {test_thread}: {len(memory.storage.get(test_thread, {}).get('messages', []))}\")\n",
    "\n",
    "# Run test\n",
    "test_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
